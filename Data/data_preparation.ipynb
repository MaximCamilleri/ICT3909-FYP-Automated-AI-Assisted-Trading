{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "#### Base Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../FinRL/\")\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import random\n",
    "import pickle\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FinRL Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Config import local_config\n",
    "from Config.local_config import (\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    "\n",
    "    DOW_30_TICKERS\n",
    ")\n",
    "\n",
    "from finrl import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature Engineering consists of a set of classes aimed at retrieving and extrapolating data. The functions can:\n",
    "1. Retrieve OHLCV data for any set of tickers\n",
    "2. Compute the values for a list of indicators on the OHLCV data\n",
    "3. Run PCA on the indicator data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_PCA():\n",
    "    def __init__(self, indicators, data, tickers) -> None:\n",
    "        self.data = data\n",
    "        self.tickers = tickers\n",
    "        self.indicators = indicators\n",
    "        self.number_of_components = -1\n",
    "    \n",
    "    def run_PCA(self, n_components = .99):\n",
    "        x = self.data.loc[:, self.indicators].values\n",
    "        x = StandardScaler().fit_transform(x)\n",
    "        pca = PCA(n_components=n_components)\n",
    "\n",
    "        principalComponents = pca.fit_transform(x)\n",
    "        principalDf = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "        pickle.dump(pca, open(f\"trained_models/PCA_Model_{str(n_components)}.pickle\", 'wb'))\n",
    "\n",
    "        ret = self.data[['date','tic','open','high','low','close','volume', 'day']]\n",
    "\n",
    "        for count in range(0, len(principalDf.columns)):\n",
    "            ret[count] = principalDf[count].values\n",
    "\n",
    "        ret['vix'] = self.data['vix']\n",
    "        ret['turbulence'] = self.data['turbulence']\n",
    "        \n",
    "        self.number_of_components = len(principalDf.columns)\n",
    "\n",
    "        ret = ret.fillna(0)\n",
    "\n",
    "        # ConfusionMatrixDisplay.from_estimator(\n",
    "        #     principalComponents, display_labels = self.indicators, xticks_rotation = \"vertical\"\n",
    "        # )\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_preparation:\n",
    "    def __init__(self, start_date, end_date, tickers, indicators) -> None:\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.tickers = tickers\n",
    "        self.indicators = indicators\n",
    "\n",
    "        self.data = YahooDownloader(start_date = self.start_date,\n",
    "                                    end_date = self.end_date,\n",
    "                                    ticker_list = tickers).fetch_data()\n",
    "        self.data.sort_values(['date', 'tic'], ignore_index = True)\n",
    "        \n",
    "\n",
    "    def add_indicators(self, vix:bool, turbulence:bool):\n",
    "        fe = FeatureEngineer(\n",
    "            use_technical_indicator = True,\n",
    "            tech_indicator_list = self.indicators, \n",
    "            use_vix = vix, # a real time market index representing the markets expectations for volatility over the next 30 days\n",
    "            use_turbulence = True, # accounts for unexpected rising and falling of the stock market\n",
    "            user_defined_feature = False)\n",
    "\n",
    "        processed = fe.preprocess_data(self.data)\n",
    "        \n",
    "        list_ticker = processed[\"tic\"].unique().tolist()\n",
    "        list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
    "        combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "        processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
    "        processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "        processed_full = processed_full.sort_values(['date','tic'])\n",
    "\n",
    "        self.data = processed_full.fillna(0)\n",
    "    \n",
    "    def add_pca(self, vix, turbulence, n_components):\n",
    "        self.add_indicators(vix, turbulence)\n",
    "        self.pca = run_PCA(self.indicators, self.data, self.tickers)\n",
    "        self.data = self.pca.run_PCA(n_components)\n",
    "    \n",
    "    def save_data(self, name):\n",
    "        self.data.to_csv(f\"../Datasets/{name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval and Saving "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator Data for DOW Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicatorData = data_preparation(TRAIN_START_DATE, TRADE_END_DATE, local_config.DOW_30_TICKERS, local_config.PCA_INDICATORS)\n",
    "indicatorData.add_indicators(vix=True, turbulence=True)\n",
    "indicatorData.save_data(name = \"DowIndicatorData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Data for DOW Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = .70\n",
    "pcaData = data_preparation(TRAIN_START_DATE, TRADE_END_DATE, local_config.DOW_30_TICKERS, local_config.PCA_INDICATORS)\n",
    "pcaData.add_pca(vix=True, turbulence=True, n_components = n_components)\n",
    "pcaData.save_data(name = f\"/PCA/Dow_Pca_{str(n_components)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Data up to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (97382, 8)\n",
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (3357, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "n_components = .99\n",
    "pcaData = data_preparation(TRAIN_START_DATE, \"2023-05-08\", local_config.DOW_30_TICKERS, local_config.PCA_INDICATORS)\n",
    "pcaData.add_pca(vix=True, turbulence=True, n_components = n_components)\n",
    "pcaData.save_data(name = f\"/PCA/Dow_Pca_{str(n_components)}_current\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (97382, 8)\n",
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (3357, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "n_components = .85\n",
    "pcaData = data_preparation(TRAIN_START_DATE, \"2023-05-08\", local_config.DOW_30_TICKERS, local_config.PCA_INDICATORS)\n",
    "pcaData.add_pca(vix=True, turbulence=True, n_components = n_components)\n",
    "pcaData.save_data(name = f\"/PCA/Dow_Pca_{str(n_components)}_current\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator Data for Dow Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicatorDataset = pd.read_csv(\"../Datasets/DOWIndicatorData.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of rows: 80301\n",
      "Sample row (2268):\n",
      "date              2010-04-26\n",
      "tic                     CSCO\n",
      "open                   27.58\n",
      "high                    27.7\n",
      "low                    27.48\n",
      "close                19.5783\n",
      "volume           3.56634e+07\n",
      "day                        0\n",
      "macd                0.306866\n",
      "boll_ub              19.7016\n",
      "boll_lb              18.2563\n",
      "rsi_30               67.1523\n",
      "cci_30               190.184\n",
      "dx_30                25.4766\n",
      "close_30_sma          18.884\n",
      "close_60_sma         18.0729\n",
      "close_60_smma        18.1605\n",
      "atr_30               7.75297\n",
      "supertrend_ub        44.0752\n",
      "supertrend_lb        4.01207\n",
      "supertrend           44.0752\n",
      "pdi                  1.70643\n",
      "mdi                  1.03068\n",
      "dx                   24.6884\n",
      "adx                  23.1065\n",
      "adxr                  25.206\n",
      "vix                    17.47\n",
      "turbulence                 0\n",
      "Name: 3370, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num of rows: {len(indicatorDataset)}\")\n",
    "rand = random.randint(0, len(indicatorDataset))\n",
    "print(f\"Sample row ({rand}):\\n{indicatorDataset.iloc[rand]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Data for DOW Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaDataset = pd.read_csv(\"../Datasets/DowPcaData.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of rows: 80301\n",
      "Sample row (36678):\n",
      "date          2015-01-09\n",
      "tic                   PG\n",
      "open               91.18\n",
      "high               91.18\n",
      "low                90.12\n",
      "close            71.6012\n",
      "volume        4.8728e+06\n",
      "day                    4\n",
      "0               -0.30608\n",
      "1              -0.747747\n",
      "2             -0.0259247\n",
      "3               -1.02958\n",
      "4              0.0249799\n",
      "5               -0.18917\n",
      "6               0.626527\n",
      "7             -0.0369904\n",
      "8              0.0133445\n",
      "9              0.0480704\n",
      "10             -0.441919\n",
      "vix                17.55\n",
      "turbulence       24.4064\n",
      "Name: 53237, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num of rows: {len(pcaDataset)}\")\n",
    "rand = random.randint(0, len(pcaDataset))\n",
    "print(f\"Sample row ({rand}):\\n{pcaDataset.iloc[rand]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-FinRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61866ea1f56d8d1253c5864ef53fda8c131f7a460e0c526de1cdeb101f9de6aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
